{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This takes the test set of claims, and for each one we save:\n",
    "- Check if claim is climate related\n",
    "- top 5 retrieved sentences\n",
    "- top 5 rereanked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iestyn/miniconda3/envs/project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIMATE_CLAIM_DETECTOR_MODEL_NAME = \"climatebert/distilroberta-base-climate-detector\"\n",
    "DENSE_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "RERANKER_MODEL_NAME_1 = \"iestynmullinor/roberta-reranker-fever-better\"\n",
    "RERANKER_MODEL_NAME_2 = \"iestynmullinor/climatebert-rereranker-f-cf-ipcc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "claim_detector = pipeline(\"text-classification\", model=CLIMATE_CLAIM_DETECTOR_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = SentenceTransformer(DENSE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reranker_tokenizer_1 = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME_1)\n",
    "reranker_model_1 = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "reranker_tokenizer_2 = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME_2)\n",
    "reranker_model_2 = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in claims from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_claims_path =  \"data/twitter_claims.csv\"\n",
    "twitter_claims = pd.read_csv(twitter_claims_path)\n",
    "twitter_claims = twitter_claims[\"claims\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in embeddings for all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of embeddings: 16004\n",
      "number of sentences: 16004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=30)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=30)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric='cosine', n_neighbors=30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/iestyn/honsProject/sentence_similarity/data/sentence_section_pairs.json', 'r', encoding='utf-8') as f:\n",
    "    SENTENCES = json.load(f)\n",
    "    SENTENCES = [sentence for (sentence, section) in SENTENCES ]\n",
    "    \n",
    "with open(f'/home/iestyn/honsProject/sentence_similarity/model_evaluation/model_embeddings/MODEL_all-mpnet-base-v2_EMBEDDINGS.pkl', 'rb') as f:\n",
    "    EMBEDDINGS = pickle.load(f)\n",
    "\n",
    "print(f\"number of embeddings: {len(EMBEDDINGS)}\")\n",
    "print(f\"number of sentences: {len(SENTENCES)}\")\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=30, metric='cosine')\n",
    "nn.fit(EMBEDDINGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Results Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dictionary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions to get results for claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_climate_change_claim(claim):\n",
    "    result = claim_detector(claim)\n",
    "    #print(result)\n",
    "    if result[0][\"label\"] == \"yes\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_30_nearest_neighbours(claim):\n",
    "    claim_embedding = dense_model.encode(claim)\n",
    "    distances, indices = nn.kneighbors([claim_embedding])\n",
    "    nearest_neighbours = [SENTENCES[i] for i in indices[0]]\n",
    "    return nearest_neighbours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_evidence(claim_sentence, evidence_sentences, reranker, tokenizer):\n",
    "\n",
    "    sentence_rerank_scores = []\n",
    "\n",
    "    for evidence_sentence in evidence_sentences:\n",
    "        tokenized_input = tokenizer(claim_sentence, evidence_sentence, padding='max_length', max_length=256, truncation=True, return_tensors=\"pt\")\n",
    "        model_output = reranker(**tokenized_input)\n",
    "\n",
    "        # get the predicted class\n",
    "        predicted_class = model_output.logits.argmax().item()\n",
    "\n",
    "        # get the probability        \n",
    "        probability = model_output.logits.max()\n",
    "\n",
    "        # if predicted score is 0, add to sentence_rerank_scores\n",
    "        if predicted_class == 0:\n",
    "            sentence_rerank_scores.append((evidence_sentence, probability))\n",
    "\n",
    "    # sort the sentence_rerank_scores by probability\n",
    "    sentence_rerank_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # remove the probability from the sentence_rerank_scores\n",
    "    sentence_rerank_scores = [x[0] for x in sentence_rerank_scores]\n",
    "\n",
    "    # return the top 5 reranked evidence sentences, if there is less than 5 in the reranked list, return however many there is\n",
    "    if len(sentence_rerank_scores) < 5:\n",
    "        return sentence_rerank_scores\n",
    "    else:\n",
    "        return sentence_rerank_scores[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results for each claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:29<00:00,  8.99s/it]\n"
     ]
    }
   ],
   "source": [
    "for claim in tqdm(twitter_claims):\n",
    "    output_dictionary[claim] = {}\n",
    "    output_dictionary[claim][\"climate_claim_detected\"] = check_for_climate_change_claim(claim)\n",
    "    \n",
    "    top_30_nearest_neighbours = get_30_nearest_neighbours(claim)\n",
    "    output_dictionary[claim][\"top_5_retrieved\"] = top_30_nearest_neighbours[:5]\n",
    "\n",
    "    top_5_reranked_1 = rerank_evidence(claim, top_30_nearest_neighbours, reranker_model_1, reranker_tokenizer_1)\n",
    "    output_dictionary[claim][\"reranked_RoBERTa-FEVER\"] = top_5_reranked_1\n",
    "\n",
    "    top_5_reranked_2 = rerank_evidence(claim, top_30_nearest_neighbours, reranker_model_2, reranker_tokenizer_2)\n",
    "    output_dictionary[claim][\"reranked_ClimateBert-f-cf-ipcc\"] = top_5_reranked_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save output dictionary as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter_claims_results.json', 'w') as f:\n",
    "    json.dump(output_dictionary, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
