{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring how many sentences are in the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16004\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"sentence_similarity/data/sentence_section_pairs.pkl\"\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    kb = pickle.load(f)\n",
    "\n",
    "print(len(kb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing how long cosine similarities take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7546026706695557\n"
     ]
    }
   ],
   "source": [
    "# generate a random vector with the 300 dimensions\n",
    "import numpy as np\n",
    "\n",
    "# calculate the cosine similarity 16000 times and measure how long it takes\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(16000):\n",
    "    cosine_similarity(np.random.rand(1, 10000), np.random.rand(1, 10000))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting training data labels are correct for reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iestyn/miniconda3/envs/project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 37.0/37.0 [00:00<00:00, 84.1kB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 77.6M/77.6M [00:16<00:00, 4.76MB/s]\n",
      "Downloading data: 100%|██████████| 6.89M/6.89M [00:02<00:00, 2.42MB/s]\n",
      "Downloading data: 100%|██████████| 6.77M/6.77M [00:01<00:00, 4.89MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:20<00:00,  6.85s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1897.88it/s]\n",
      "Generating train split: 208346 examples [00:00, 383847.84 examples/s]\n",
      "Generating validation split: 19998 examples [00:00, 402607.77 examples/s]\n",
      "Generating test split: 19998 examples [00:00, 402042.34 examples/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 208346/208346 [00:29<00:00, 6996.79 examples/s]\n",
      "Map: 100%|██████████| 19998/19998 [00:01<00:00, 13324.00 examples/s]\n",
      "Map: 100%|██████████| 19998/19998 [00:01<00:00, 13097.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    ")\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "model_id = \"climatebert/distilroberta-base-climate-f\"\n",
    "\n",
    "\n",
    "# relace the value with your model: ex <hugging-face-user>/<model-name>\n",
    "repository_id = \"iestynmullinor/climatebert-rerank-fever\"\n",
    "\n",
    "training_data_path = \"iestynmullinor/fever_reranker_training\"\n",
    "\n",
    "dataset = load_dataset(training_data_path)\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "dev_data = dataset[\"validation\"]\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "def tokenize(batch):\n",
    "    claims = list(map(str, batch[\"claim\"]))\n",
    "    evidences = list(map(str, batch[\"evidence\"]))\n",
    "    tokenized_inputs = tokenizer(claims, evidences, padding=True, truncation=True, max_length=256)\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_data= train_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
    "test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n",
    "dev_data = test_data.map(tokenize, batched=True, batch_size=len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claim', 'evidence', 'label', 'input_ids', 'attention_mask']\n",
      "{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'evidence': 'The Fox Broadcasting Company ( often shortened to Fox and stylized as FOX ) is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox . Nikolaj Coster-Waldau . He then played Detective John Amsterdam in the short-lived Fox television series New Amsterdam ( 2008 ) , as well as appearing as Frank Pike in the 2009 Fox television film Virtuality , originally intended as a pilot . He became widely known to a broad audience for his current role as Ser Jaime Lannister , in the HBO series Game of Thrones .', 'label': 0, 'input_ids': [0, 38334, 1168, 1176, 230, 13991, 12, 771, 5618, 1180, 1006, 19, 5, 2063, 13610, 1260, 4, 2, 2, 133, 2063, 13610, 1260, 36, 747, 30288, 7, 2063, 8, 15240, 1538, 25, 7481, 4839, 16, 41, 470, 2370, 2777, 1861, 2308, 2384, 1546, 14, 16, 2164, 30, 5, 2063, 5528, 826, 8540, 9, 733, 620, 9348, 2063, 479, 27998, 1176, 230, 13991, 12, 771, 5618, 1180, 479, 91, 172, 702, 13924, 610, 16342, 11, 5, 765, 12, 18062, 2063, 2384, 651, 188, 16342, 36, 2266, 4839, 2156, 25, 157, 25, 8165, 25, 3848, 20859, 11, 5, 2338, 2063, 2384, 822, 21741, 1571, 2156, 3249, 3833, 25, 10, 4792, 479, 91, 1059, 3924, 684, 7, 10, 4007, 2437, 13, 39, 595, 774, 25, 6251, 23929, 37773, 4742, 2156, 11, 5, 10240, 651, 2436, 9, 13480, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "['<s>', 'Nik', 'ol', 'aj', 'ĠC', 'oster', '-', 'W', 'ald', 'au', 'Ġworked', 'Ġwith', 'Ġthe', 'ĠFox', 'ĠBroadcasting', 'ĠCompany', '.', '</s>', '</s>', 'The', 'ĠFox', 'ĠBroadcasting', 'ĠCompany', 'Ġ(', 'Ġoften', 'Ġshortened', 'Ġto', 'ĠFox', 'Ġand', 'Ġstyl', 'ized', 'Ġas', 'ĠFOX', 'Ġ)', 'Ġis', 'Ġan', 'ĠAmerican', 'ĠEnglish', 'Ġlanguage', 'Ġcommercial', 'Ġbroadcast', 'Ġtelevision', 'Ġnetwork', 'Ġthat', 'Ġis', 'Ġowned', 'Ġby', 'Ġthe', 'ĠFox', 'ĠEntertainment', 'ĠGroup', 'Ġsubsidiary', 'Ġof', 'Ġ21', 'st', 'ĠCentury', 'ĠFox', 'Ġ.', 'ĠNikol', 'aj', 'ĠC', 'oster', '-', 'W', 'ald', 'au', 'Ġ.', 'ĠHe', 'Ġthen', 'Ġplayed', 'ĠDetective', 'ĠJohn', 'ĠAmsterdam', 'Ġin', 'Ġthe', 'Ġshort', '-', 'lived', 'ĠFox', 'Ġtelevision', 'Ġseries', 'ĠNew', 'ĠAmsterdam', 'Ġ(', 'Ġ2008', 'Ġ)', 'Ġ,', 'Ġas', 'Ġwell', 'Ġas', 'Ġappearing', 'Ġas', 'ĠFrank', 'ĠPike', 'Ġin', 'Ġthe', 'Ġ2009', 'ĠFox', 'Ġtelevision', 'Ġfilm', 'ĠVirtual', 'ity', 'Ġ,', 'Ġoriginally', 'Ġintended', 'Ġas', 'Ġa', 'Ġpilot', 'Ġ.', 'ĠHe', 'Ġbecame', 'Ġwidely', 'Ġknown', 'Ġto', 'Ġa', 'Ġbroad', 'Ġaudience', 'Ġfor', 'Ġhis', 'Ġcurrent', 'Ġrole', 'Ġas', 'ĠSer', 'ĠJaime', 'ĠLann', 'ister', 'Ġ,', 'Ġin', 'Ġthe', 'ĠHBO', 'Ġseries', 'ĠGame', 'Ġof', 'ĠThrones', 'Ġ.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.column_names)\n",
    "print(train_data[0])\n",
    "print(tokenizer.convert_ids_to_tokens(train_data[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling data for reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claim', 'evidence', 'label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 208346/208346 [00:00<00:00, 509407.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claim', 'evidence', 'label']\n",
      "{'claim': 'Singapore lies 137 km or one degree north of the equator.', 'evidence': \"Singapore . It lies one degree ( 137 km ) north of the equator , at the southern tip of peninsular Malaysia , with Indonesia 's Riau Islands to the south .\", 'label': 0}\n",
      "{'claim': 'Louis C.K. took a hiatus in 2016.', 'evidence': 'Louis C.K. . During an extended Louie hiatus , C.K. created and starred in his web series Horace and Pete in 2016 , and voiced the lead role in the animated film The Secret Life of Pets the same year .', 'label': 1}\n",
      "{'claim': 'Hugh Jackman plays Wolverine.', 'evidence': 'Hugh Jackman . He is known for his long-running role as Wolverine in the X-Men film series , as well as for his lead roles in films such as the romantic-comedy fantasy Kate & Leopold ( 2001 ) , the action-horror film Van Helsing ( 2004 ) , the magic-themed drama The Prestige ( 2006 ) , the epic fantasy drama The Fountain ( 2006 ) , the epic historical romantic drama Australia ( 2008 ) , the film version of Les Misérables ( 2012 ) , and the thriller Prisoners ( 2013 ) .', 'label': 0}\n",
      "Number of samples for class 0:  30445\n",
      "Number of samples for class 1:  30445\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    \n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "UNDERSAMPLE=True\n",
    "\n",
    "model_id = \"climatebert/distilroberta-base-climate-f\"\n",
    "\n",
    "\n",
    "# relace the value with your model: ex <hugging-face-user>/<model-name>\n",
    "repository_id = \"iestynmullinor/climatebert-rerank-fever\"\n",
    "\n",
    "training_data_path = \"iestynmullinor/fever_reranker_training\"\n",
    "\n",
    "dataset = load_dataset(training_data_path)\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "dev_data = dataset[\"validation\"]\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "def tokenize(batch):\n",
    "    claims = list(map(str, batch[\"claim\"]))\n",
    "    evidences = list(map(str, batch[\"evidence\"]))\n",
    "    tokenized_inputs = tokenizer(claims, evidences, padding=True, truncation=True, max_length=256)\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(train_data.column_names)\n",
    "\n",
    "# remove all examples where the evidence is None\n",
    "\n",
    "train_data = train_data.filter(lambda example: example[\"evidence\"] is not None)\n",
    "\n",
    "if UNDERSAMPLE:\n",
    "    # Convert the DataFrame to a pandas DataFrame\n",
    "    train_data_pd = pd.DataFrame(train_data)\n",
    "\n",
    "    # Count the number of instances in each class\n",
    "    class_counts = train_data_pd['label'].value_counts()\n",
    "\n",
    "    # Find the number of instances in the minority class\n",
    "    minority_class_count = class_counts.min()\n",
    "\n",
    "    # Perform undersampling\n",
    "    undersampled_data = pd.concat(\n",
    "        [train_data_pd[train_data_pd['label'] == label].sample(n=minority_class_count, random_state=42) for label in class_counts.index]\n",
    "    )\n",
    "\n",
    "    # Shuffle the undersampled data\n",
    "    undersampled_data = shuffle(undersampled_data, random_state=42)\n",
    "\n",
    "    # Convert the undersampled data back to a Hugging Face dataset\n",
    "    train_data = datasets.Dataset.from_dict(undersampled_data.to_dict('list'))\n",
    "\n",
    "    print(train_data.column_names)\n",
    "    print(train_data[0])\n",
    "    print(train_data[1])\n",
    "    print(train_data[2])\n",
    "\n",
    "    train_data_df = pd.DataFrame(train_data)\n",
    "\n",
    "    # print the number of samples for class 0 and class 1\n",
    "    print(\"Number of samples for class 0: \", len(train_data_df[train_data_df[\"label\"] == 0]))\n",
    "    print(\"Number of samples for class 1: \", len(train_data_df[train_data_df[\"label\"] == 1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
