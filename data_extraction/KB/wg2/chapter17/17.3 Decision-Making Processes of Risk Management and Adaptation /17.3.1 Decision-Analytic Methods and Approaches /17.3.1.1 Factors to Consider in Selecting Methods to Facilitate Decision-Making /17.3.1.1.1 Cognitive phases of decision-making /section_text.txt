The decision process often involves overlapping and iterative development of the components leading towards a decision, resulting in the blurring of stages but involving different phases of cognitive activity (Figure 17.7; Holtzman, 1989; French, 2015; French, 2020). Framing the problem (Orlove et al., 2020), by modelling its relationships with the human and natural systems and eliciting objectives, values and scope of the problem from stakeholders, is a precursor to analyses of options but may be returned to whenever a phase of ‘sense-making and modelling’ is required (high confidence) (Ackermann, 2012; Keeney, 2012; Slotte and Hämäläinen, 2014; Abbas and Howard, 2015; Marttunen et al., 2017; Korhonen and Wallenius, 2020; French, 2021).Figure 17.7 | Relationships between different processes of decision-making to manage climate-related risks in the real world, noting that, when appropriate, some aspects may only require experience to be re-used.  (1) Formulation of risks of concern and accompanying policies and objectives for managing those risks, forming prescriptive models for the decision maker. (2) Knowledge, understanding and observations of the real world are used to assess past and current impacts and future risks using descriptive models, based on the perspectives and prescriptive models arising from (1). If not well formulated from other experience, processes in (1) and (2) interact to make sense of the world and what needs to be done. In iterative management, (1) and (2) also form the basis for monitoring, reviewing and evaluating effectiveness of adaptations. (3) Use of decision-support and decision-analytic tools to appraise costs and benefits of different options for ameliorating future risks. The double-headed arrow indicates where two-way interactions occur between different activities (likely to be iterative, feedback and nonlinear processes); modelling and assessments are repeated and revised in tandem with the planning and evaluation of options, based on interactions with the policymakers and stakeholders. (4) The decision maker, which may be a group of people, interacts with the evaluation of options (two-way interaction) and interprets the efficacy of the options and the implications for the real world, ultimately choosing one or more actions to satisfy the policy objectives to manage the risks. (5) Implementation of the actions in the real world, which may be once-only actions or instigation of a feedback management system that enables ongoing adjustments to meet objectives.Open figure The cognitive phase of ‘analysing and exploring’ uses models and existing data and/or knowledge services as available to explore the relevance/efficacy of adaptations to ameliorate risk or to meet other adaptation objectives, as well as possible flow-on effects of those actions (Section 17.3.1.4). Sensitivity and robustness analyses can be useful if conditions are favourable to supplement the decision analysis, setting bounds on some of the residual uncertainty (high confidence) (Borgonovo and Plischke, 2016; Ferretti et al., 2016). Validation of models and verification of data (Tittensor et al., 2018) are becoming highlighted as important steps in this phase or in the sense-making phase, particularly in their capacity to understand and test decision makers and stakeholders’ perceptions (medium confidence). Randomisation methods, Bayesian methods, interval methods, multi-criteria decision analysis (MCDA), decision-making under deep uncertainty (DMDU) and economic and financial approaches (e.g., Real Options Analysis) are tools of choice in this phase (high confidence) (Table 17.4) (Abbas and Howard, 2015; Bendoly and Clark, 2016; Borgonovo and Plischke, 2016; Iooss and Saltelli, 2017; Korhonen and Wallenius, 2020; Saltelli et al., 2020). Decision-support tools in the provision of data and/or modelling methods are regularly used in this and the sense-making phase (high confidence) (Section 17.3.1.2).The phase of interpreting the analyses to make decisions on climate adaptation followed by implementation is the least described in the literature (Figure 17.8). Decision process management tools and methods for communicating choices, outcomes and implementation are expected to be used to provide support in this phase, particularly for understanding whether the advice is fit for purpose, and the efficacy of choices are clear (low confidence) (Spetzler et al., 2016).Figure 17.8 | Decision-analytic tools used across different geo-political scales and how they contributed to decision outcomes. Points comprise the type of decision-making body (C = Community; G = Government; B = Business/Industry; F = Finance; N = NGO; A = All categories) coupled with the reference number in square brackets, which correspond to numbered references in the case studies of Table 17.4. Colours of the points correspond to the class of decision-analytic tool: Bayesian (red), DMDU (decision-making under deep uncertainty) (brown), decision process management (dark blue), economic and financial methods (purple), interval methods (light blue), MCDA—full ranking (light green) or partial ranking (dark green), soft elicitation (Black).Open figure Table 17.4 | Characteristics of the main approaches to decision analysis with respect to their Cynefin context, the manner in which they can be used to address different uncertainties, where they may be used in different cognitive phases of the decision-making process, the resources required and some case studies for further exploring how they might be used. Numbers in square brackets after references in case studies refer to the references plotted in Figure 17.8.A: Bayesian methods (Keeney and Raiffa, 1993; Smith, 2010; Gelman et al., 2013; Reilly and Clemen, 2013; Abbas and Howard, 2015; Sperotto et al., 2017; Marchau et al., 2019)  A structured approach to assembling information around the consequences of choices, either by modelling, by analysis of multiple scenarios or by structuring deliberation; underpinned by a theoretical base, coherent assumptions and powerful computational methods; can use both observational data and expert knowledge, weighting them appropriately; same approaches as in artificial intelligence algorithms. Biases (information, stakeholders, decision makers) can be made explicit. Traditionally, Bayesian methods computationally identify an ‘optimal’ decision, based on maximising the expected utility across a number of specified requirements, represented as functions. Examples include the general application of decision network models (Richards et al., 2013; Sperotto et al., 2017); the use of decision network analyses based on elicitation to choose adaptations to coastal management in a lagoonal area in Italy (Catenacci and Giupponi, 2013) and coastal community in UK (Jäger et al., 2018); combination of economic models and decision models to assess research and development priorities (Baker and Solak, 2011); combining outputs from models, observations and opinions in a decision framework for assessing climate impacts on water nutrient loads in Italy (Sperotto et al., 2019) and a general review for water resource management (Phan et al., 2019); combining results from different dynamic models to assess human mortality from ozone in the USA (Alexeeff et al., 2016); assessing adaptive capacity of surf lifesaving in Australia (Richards et al., 2016); and assessing urban flood risks in Denmark (Åström et al., 2014).Cognitive phaseResources requiredCase studiesSense-making and modellingAnalysing and exploringInterpreting and implementingConstruction of hierarchical models, belief nets (Sperotto et al., 2017; Phan et al., 2019), decision trees (Keeney and Raiffa, 1993) and influence diagrams (Keeney and Raiffa, 1993; Reilly and Clemen, 2013) supplemented by many soft elicitation techniques helps build models for quantitative analysis (Gelman, 2003; Bendoly and Clark, 2016).Bayesian updating and expected utility analysis supplemented by robustness and sensitivity analyses (Rios Insua, 1999; Rios Insua and Ruggeri, 2000; French et al., 2009; Smith, 2010; Reilly and Clemen, 2013; Abbas and Howard, 2015).Use of graphical models (decision trees, belief nets and influence diagrams) and sensitivity plots can help make transparent and explain reasoning for strategy to stakeholders and implementers (Bendoly and Clark, 2016) and provide for auditable building of consensus.Bayesian decision-analytic models can be applied with increasing complexity and sophistication to any given problem. Coherence between different levels of sophistication can be maintained. Thus, the resources can be tailored to the time and support available for the analysis. The most sophisticated analyses are computationally demanding.Alexeeff et al. 2016) [1], Åström et al. (2014) [2], Baker and Solak (2011) [3], Catenacci and Giupponi (2013) [4], Jäger et al. (2018) [5], Phan et al. (2019) [6], Richards et al. (2013) [7], Richards et al. (2016) [8], Sperotto et al. (2017) [9], Sperotto et al. (2019) [10]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticAll can be modelled probabilistically, perhaps supplemented by sensitivity analysis (Rios Insua, 1999; Rios Insua and Ruggeri, 2000; Iooss and Saltelli, 2017). Deep uncertainties can be investigated via scenarios (French, 2020).Uncertainties resolved or reduced by discussion, then values modelled by multi-attribute values and utilities (Keeney, 1992; Keeney and Raiffa, 1993; Gregory et al., 2012). Residual uncertainties explored via sensitivity analysis.Any stochastic uncertainties modelled probabilistically; otherwise, deterministic modelling with sensitivity analysis. Value functions tend to be used more than utility functions (Keeney and Raiffa, 1993; Goodwin and Wright, 2014).Epistemic uncertainties updated via Bayesian statistics/machine learning, then remaining stochastic uncertainties modelled probabilistically. Full Bayesian decision modelling possible (French et al., 2009; Smith, 2010; Abbas and Howard, 2015).More exploratory analysis (Gelman, 2003) to understand behaviours with less complex Bayesian modelling support by sensitivity and robustness studies (Rios Insua, 1999; French, 2003). Scenario-focused decision analysis to cope with deep uncertainties (French, 2020). Careful deliberations to construct values and utilities. (Keeney and Raiffa, 1993; Gregory et al., 2012).Formal modelling impossible. Much exploratory work to identify potential causes and effects. Little if any complex analysis.B: Decision-making under deep uncertainty (DMDU)   (Hallegatte et al., 2012; Weaver et al., 2013; Marchau et al., 2019; Workman et al., 2021)  Deep uncertainty relates to circumstances in which data are too sparse, experts are in too much disagreement or time is too short to model the uncertainty. As such, DMDU methods are focused on working in theCynefinComplex Space context. Approaches emphasise robustness (‘no regrets’ options) and the use of scenarios, and often link well with scenario-focused robust Bayesian studies (Cross-Chapter Box DEEP in this Chapter). DMDU studies draw in many other approaches to decision analysis, using them to identify robust rather than optimal strategies, as in robust decision-making (RDM). DMDU analyses can help decision makers to think contingently and build a more wide-ranging recognition of the risks. They often integrate with other classes of tools. Examples include RDM for hydro-power design using down-scaled climate data in Sub-Saharan Africa (Taner et al., 2017), RDM for water management in California, USA (Lempert and Groves, 2010), the Colorado River, USA, and for international climate investment strategies (Groves et al., 2019), use of decision scaling (Brown et al., 2019), comparison of RDM and Info-gap methods (Hall et al., 2012) and review of using climate modelling in RDM (Weaver et al., 2013).Cognitive phaseResources requiredCase studiesSense-making and modellingAnalysing and exploringInterpreting and implementingSome of the simpler DMDU tools complement soft elicitation tools and can help to identify relevant scenarios and help formulate problems.Many Bayesian or MCDA tools can be used here but with DMDU’s additional emphasis on robustness and the exploration of several/many scenarios.DMDU with its emphasis on robustness encourages contingency planning in implementation with careful monitoring to identify emerging risks.Some of the simpler models do not require substantial resources, but the application of parallel sophisticated analyses in several scenarios can be computationally demanding. Also, the emphasis on discussion of robustness can be demanding on the time of problem-owners, experts and stakeholders.Brown et al. (2019) [11], Groves et al. (2019) [12], Hall et al. (2012) [13], Lempert and Groves (2010), [14], Taner et al. (2017) [15], Weaver et al. (2013) [16]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticMethods are designed for deep epistemic uncertainties. Some can deal with stochastic uncertainties. Analytical uncertainties seldom accounted for.Some DMDU methods draw on MCDA methods and thus consider ambiguity and value uncertainties. In any case, DMDU methods support wide deliberation with stakeholders.Deep uncertainty is absent, but the principles and processes of decision-making may be used.Deep uncertainty is absent, but the principles of decision-making may be used.The complex and chaotic spaces are home to deep uncertainties. DMDU tools and more particularly processes are relevant here. The emphasis on robustness is very relevant. The tools themselves are relatively simply structured but are effective at stimulating discussion.Deep uncertainties are rife in the chaotic contexts. DMDU emphases on robustness and possible scenarios can stimulate creative discussions of ill-understood issues.C: Decision process management  (Raz and Micheal, 2001; Dalkir, 2005; Burstein and W. Holsapple, 2008; Jashapara, 2011; Bonczek et al., 2014; Sauter, 2014; Holsapple et al., 2019)  A range of tools and techniques to help manage the decision-making process and support risk management and the implementation of the chosen strategy. Some tools organise data and analyses, often being built on a geographic information system, known as decision support tools. Others manage processes, organising workflows. Some have inevitably expanded in function to support decision-making itself, even though their primary focus might be on, say, implementation and monitoring risks. Such tools are closely related to knowledge management systems; knowledge management processes and decision process management differ more in terminology than in substance. Examples include tools for agriculture (Biehl et al., 2017), evaluating and comparing CMIP climate models (Parding et al., 2020), development of action cycles (Park et al., 2012) and decision support systems across a range of sectors and decision-group applications (Papathanasiou et al., 2016).Cognitive phaseResources requiredCase studiesSense-making and modellingAnalysing and exploringInterpreting and implementingProcess, project, knowledge elicitation and risk management tools help identify how to structure decision-making processes. Decision process tools can capture details for implementation and document process for audit trail.Tools help structure decision-making processes and ensure timely involvement of problem owners, stakeholders, and experts. Knowledge management tools can capture details for implementation and document process for audit trail.Project management tools plan implementation and risk management tools identify what to monitor during implementation. Knowledge management tools maintain audit trail and track reasoning for choices made during implementation.Decision process management tools can reduce resources needed in the decision-making process. However, this assumes that the tools are already installed on local information systems and that the analysis team is experienced in using them. Otherwise, resource is needed to understand and train in the use of the tools.Biehl et al. (2017) [17], Papathanasiou et al. (2016) [18], Parding et al. (2020) [19], Park et al. (2012) [20]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticNot designed to address uncertainties involved in the decision itself, but may handle project risks in the decision process, especially implementation.Not usually addressed, since ambiguities and value uncertainties will be addressed in the decision-making itself, but may use those values in risk management of implementation.Simple project management tools may be sufficient here.Project management and risk management tools apply easily here.Project management and risk management tools may be used, but attention needs to be paid to risks that are complex in nature with little knowledge of precise relationships between cause and effects.Project management and risk management tools may be used, but attention needs to be paid to risks that are complex in nature with little knowledge of precise relationships between cause and effects.D: Economic and financial methods (Howell et al., 2001; Pearce et al., 2006; Boardman et al., 2017; Atkinson et al., 2018a; Hurlbert et al., 2019)  Stem from economic theory and accounting practices: for example, cost–benefit analysis, which seeks to price out all aspects of the consequence of a strategy, portfolio analysis, or real options theory, which seeks to value financial investments allowing for their risks and the contingent buying and selling. Such methods are perceived as objective when dealing with tangibles, but are more controversial in their valuing of intangibles. Since these methods model uncertainties with probabilities and then work with expectations, they share much in common with Bayesian methods. However, many applications of cost–benefit analysis omit any detailed treatment of uncertainty. Examples examine the economic costs and benefits of adaptation pathways for storm water infrastructure in Singapore (Manocha and Babovic, 2017), and a coastal mega city, Los Angeles in the USA (de Ruig et al., 2019)Cognitive phaseResources requiredCase studiesSense-making and modellingAnalysing and exploringInterpreting and implementingIn themselves, these methods do not support sense-making and modelling, though discussions of how to value impacts, both tangible and intangible can be catalytic in understanding the issues.These tools focus mainly on analysis and evaluating the costs and benefits of various options. They are not designed to be used interactively so are more often deployed and communicated via reports than interactive workshops.Since community-based adaptation (CBA) methods do not emphasise the analysis of uncertainties and risks, they are less suited for use in developing and communicating an implementation plan. Real options with their emphasis on contingency are much more suited (Fischhoff, 2015).Cost–benefit analysis for complex projects is a major undertaking, with much data collection needed to value outcomes. Real options also require data on risks and uncertainties. Both may have high computational needs.de Ruig et al. (2019) [21], Manocha and Babovic (2017) [22]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticCost–benefit methods usually deal with uncertainty via expectations with little attention to probability distributions; real options methods tend to treat uncertainty in much more sophisticated ways. Both methods, when applied fully have many points of contact with Bayesian methods (Neely and de Neufville, 2001; Bedford et al., 2005)These methods reduce all value and preference information to financial equivalents. The key issue is to find a market in which all outcomes may be valued financially. Modern CBA methods use much more subtle techniques for this than those applied in the last century (Bedford et al., 2005; Saarikoski et al., 2016).Although CBA and many financial methods work in theory, the complexity makes them seldom worth the effort.The methods may be applied to evaluate complex projects, but CBA tends to ‘average out’ rather than analyse uncertainty.The recognition of the need to treat deep uncertainties using real options has been investigated (Hallegatte et al., 2012; Buurman and Babovic, 2016).Formal modelling impossible. Much exploratory work to identify potential causes and effects. Little if any complex analysis.E: Interval methods (Shafer, 1976; Pedrycz et al., 2011)  Because of concerns that the statistical accuracy of some data is unknown, and that decision makers and experts cannot make numerical judgements accurately, analyses have been suggested which work with ranges of values in categories (intervals) as their inputs. While avoiding accuracy issues, weakening the arithmetic may result in other foundational assumptions not being met, including some basic principles of rationality. Different types of uncertainty can often be confused, and the analyses can contradict basic probability theory. Interval models of semantics and imprecision can be useful in exploring ambiguity and value uncertainty, though modelling rather than resolving such uncertainties does not necessary help in decision-making. Some interval methods can be thought of more as sensitivity techniques applied to other decision-analytic approaches. Typical approaches here relate to the fuzzy or possibility theory, and evidential reasoning. Examples include using fuzzy methods to assess climate adaptations in ports in China (Yang et al., 2018), water supply vulnerability in South Korea (Kim and Chung, 2013) and resilience of the Nile River Delta (Batisha, 2015); and evidential reasoning in an environmental impact assessment for flood mitigation in Manila Philippines (Gilbuena et al., 2013).Cognitive phaseResources requiredCase studiesSense-making and modellingAnalysing and exploringInterpreting and implementingThe emphasis on modelling ambiguity may help structure a model initially, but the lack of structures to model and explore complex interdependencies may inhibit the ability to build a valid representation of the issues.If there are substantial data available, then even the simplest of these methods can produce useful results. But with small quantities of data, their data analysis may be too inefficient. Evidential reasoning MCDA can be insightful on the preference side.The emphasis on linguistic uncertainty may in some cases mask some of the issues (French, 1995).Many methods are rather simple in application and require only moderate resources, but they may face issues in scaling up to major complex problems.Batisha (2015) [23], Gilbuena et al. (2013) [24], Kim and Chung (2013) [25], Yang et al. (2018) [26]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticThere are issues of operational definition of quantities in some methodologies. Some simpler interval methods have no concept of conditionality so cannot model learning effectively, but there are some very sophisticated theories of evidence that can. Interval methods can also provide sensitivity analyses for Bayesian and MCDA methods (Shafer, 1976; Rios Insua, 1990).Some methods can be simplistic, with quantities not being operationally defined. The evidential reasoning approach to MCDA allows exploration of the relative weights on different criteria or between levels in criteria (Xu, 2012; Zhang et al., 2017).Methods can be applied here without major issue, possibly because the simple, repetitive nature of the problem allows access to much data and the possibility of tuning the methods to the application.Since the methods often capture rather than explore and resolve ambiguity and value uncertainties, they can hide issues. Also, the lack, in some cases, of operational definitions may mean that some quantification is dubious. Evidential reasoning methods can help analyse conflicting objectives (French, 1995; Xu, 2012).The recognition of the need to treat deep uncertainties using real options has been investigated (Hallegatte et al., 2012; Buurman and Babovic, 2016).The ability to deal with ambiguity may be helpful in poorly understood situations, but the emphasis on capturing ambiguity may ultimately slow the building of understanding.F: Multi-criteria decision analysis (MCDA): Full ranking and optimal seeking (Bell et al., 2001; Belton and Stewart, 2002; Bouyssou et al., 2006; Zopounidis and Pardalos, 2010; Tzeng and Huang, 2011; Velasquez and Hester, 2013; Kumar et al., 2017)  Covers many approaches: indeed, Bayesian, DMDU and interval methods are sometimes considered MCDA. Some MCDAs seek an optimal or best strategy; others form partial rankings, eliminating weak strategies but not discriminating fully between the better ones. Many MCDA methods eschew dealing with uncertainties and focus on modelling and exploring conflicting objectives and balancing these. MCDA techniques are especially useful in working with senior decision makers in setting policy and broad objectives, and in processes of stakeholder engagement. Examples include ranking adaptation and mitigation priorities at a national level in the Netherlands (de Bruin et al., 2009), Lithuania (Streimikiene and Balezentis, 2013) and Bangladesh (Haque, 2016), in the forestry sector in Nicaragua (Guillén Bolaños et al., 2018) and in emissions trading in the European Union (Konidari and Mavrakis, 2007).Cognitive phaseResources requiredCase studiesSense-making and modellingAnalysing and exploringInterpreting and implementingThere is growing experience in combining soft elicitation with tools to formulate problems (Marttunen et al., 2017). Many MCDA tools naturally encourage discussion and deliberation on developing appropriate value structures. However, exploration and formulation of stochastic and epistemological uncertainties is less developed (Durbach and Stewart, 2020a).Emphasis is usually on analysing and exploring, resolving conflicting objectives. MCDA methods come into their own at this stage of the process. Sensitivity tools and intuitive graphical displays exist for many of the methods (Gunawan and Azarm, 2005; Boardman et al., 2017).Use of graphical models and sensitivity plots can help explain reasoning for strategy to stakeholders and implementers (Bendoly and Clark, 2016).The more exploratory methods can be quite light in terms of computational resource, but require interactions with decision makers and stakeholders in workshops. Methods with use complex stochastic mathematical programming can be computationally demanding and require substantial data.(de Bruin et al., 2009) [27], (Guillén Bolaños et al., 2018) [28], (Haque, 2016) [29], (Konidari and Mavrakis, 2007) [30], (Streimikiene and Balezentis, 2013) [31]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticThese methods tend to focus on balancing and resolving conflicting objectives and include little or no analysis of stochastic and epistemic uncertainties. Interactive methods that use complex objective functions do need to consider convergence criteria for analytic uncertainties.Many methods here use multi-attribute value functions and focus on using weights to explore different emphases on conflicting objectives. One very popular method is analytic hierarchy processing (AHP) (Saaty, 1980) though this has issues in scaling up to evaluate more than a handful of policies.Usually in the known context, the objective function is well understood; but in cases where it is not, interactive multi-objective programming can offer a way forward (Klamroth et al., 2018).If the objective function is not well understood, then these methods can be useful and can be extended to stochastic programming, but epistemic uncertainties are not really addressed (Gutjahr and Pichler, 2016).Methods can explore conflicting objectives, but seldom are able to address deep epistemic uncertainties, unless combined with scenarios (Stewart et al., 2013; Marchau et al., 2019; Durbach and Stewart, 2020a).Formal modelling impossible. Much exploratory work to identify potential causes and effects. Little if any complex analysis.G: Multi-criteria decision analysis (MCDA): Partial ranking (Roy, 1996; Bell et al., 2001; Belton and Stewart, 2002; Bouyssou et al., 2006; Behzadian et al., 2010; Zopounidis and Pardalos, 2010; Tzeng and Huang, 2011; Bouyssou and others, 2012; De Smet and Lidouh, 2012; Velasquez and Hester, 2013; Figueira et al., 2016; Govindan and Jepsen, 2016)  Examples include developing criteria for assessing climate protection strategies and applying these to retrofitting a school to manage climate risks in Germany (Markl-Hummel and Geldermann, 2014); evaluating outranking approaches for managing heat stress in a large city in Australia (El-Zein and Tonmoy, 2015); using MCDA to manage the interactions of climate change with tourism in Greece (Michailidou et al., 2016); and identifying priorities to manage droughts and floods in agriculture in Bangladesh (Xenarios and Polatidis, 2015).Cognitive phaseResources RequiredCase StudiesSense-making and modellingAnalysing and exploringInterpreting and implementingGraphical representations of partial orders are useful in model formulation, and the emphasis on exploring what can be said objectively about dominance relations can build a kernel of consensus between decision makers and stakeholders.ELECTRE and PROMETHEE implementations of outranking approaches have many tools for exploring partial relations and analysing agreements and the reasoning behind these.The analysis of dominance can provide a sound footing for building risk registers to aid implementation. Understanding the kernel of consensus can also aid communication.If an outranking algorithm is essentially combinatorial in its approach, then for complex problems there may be computational problems. Some of the methods may require less interaction with decision-makers and stakeholders if they can deduce many partial relations from objective data.(El-Zein and Tonmoy, 2015) (Markl- [32], Hummel and Geldermann, 2014) [33], (Michailidou et al., 2016) [34], (Xenarios and Polatidis, 2015) [35]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticModelling of all forms of uncertainty including epistemic uncertainty is not the primary objective of these methods. Stochastic uncertainty may be included as probability distributions, but there is no formalism for learning to address epistemic uncertainties (Hyde et al., 2003; Behzadian et al., 2010; Gervásio and Simões da Silva, 2012).Partial ranking or outranking methods seek, first of all, to identify dominance between options and preference relations that can be agreed somewhat objectively. Thus, first they eliminate suboptimal alternatives before seeking a fuller ranking. Ambiguity and value uncertainty may also be quantified (Behzadian et al., 2010; Figueira et al., 2016; Govindan and Jepsen, 2016).Usually in the known context, the objective function is well understood; but when it is not, outranking methods can identify a partial ranking without needing too many interactions with problem-owners.Since epistemic uncertainties are not fully addressed, these methods can only help in relation to conflicting objectives, but robustness to uncertainties will need addressing (Hyde et al., 2003).Outranking methods may be combined with scenarios to explore and analyse decisions under deep uncertainty (Hyde et al., 2003; Durbach, 2014).Formal modelling impossible. Much exploratory work to identify potential causes and effects. Little if any complex analysis.H: Soft elicitation (Rosenhead and Mingers, 2001; Shaw et al., 2006; Shaw et al., 2007; Ackermann, 2012; Bendoly and Clark, 2016)  Also known as problem structuring, it is the process of asking problem owners, experts and stakeholders for the knowledge, perceptions, beliefs, uncertainties and values that a model needs to embody before being populated with numbers. Methods here help in problem formulation, structuring understanding: for example, cognitive maps, soft operational research diagrams, soft systems, prompts such as PESTLE and other qualitative tools (Prober et al., 2017; Symstad et al., 2017). The output of soft elicitation can lead to the building of sophisticated quantitative models (Symstad et al., 2017) and can also structure communications and deliberations with stakeholders. Exploratory data analysis and visual analytics are also relevant. Soft elicitation has enormous advantages in setting the frame for communication between all parties (Prober et al., 2017); there are many cases in which the clarity brought by framing the issues well has obviated the need for formal quantitative analysis. Examples include Adaptation Pathway planning and elicitation on managing a national park in the USA (Symstad et al., 2017), poverty alleviation in a province in Indonesia (Butler et al., 2016), woodland landscapes in Australia (Prober et al., 2017) and general considerations for contested adaptations (Bosomworth et al., 2017).Cognitive phaseResources requiredCase StudiesSense-making and modellingAnalysing and exploringInterpreting and implementingSoft elicitation tools provide much support to sense-making, formulating problems and identifying relevant issues to be addressed (Shaw et al., 2006; Shaw et al., 2007; Ackermann, 2012).Soft elicitation is not relevant to quantitative analysis and evaluation per se, but can support the exploration of residuals to understand the quality of the models and detect further factors to be addressed.The results of soft elicitation provide the dimensions for communication by identifying the issues that are important to stakeholders and building understanding in those implementing the policies.Physical resources requirements are relatively slight: sometimes post-its and a white board can be sufficient, though modern visual analytics can require substantial computing resource. However, the demands on the time of problem owners, stakeholders and experts can be significant.(Bosomworth et al., 2017) [36], (Butler et al., 2016) [37], (Prober et al., 2017) [38], (Symstad et al., 2017) [39]UncertaintiesCynefin context Stochastic, epistemic, analytical (descriptive modelling)  Ambiguityvalue (prescriptive modelling)  KnownKnowableComplexChaoticSoft elicitation tools are available to elicit problem-owners’ and experts’ perceptions of these uncertainties and, more particularly, dependences and independences between them. Exploratory data analysis is also relevant (Steed et al., 2013; Bendoly and Clark, 2016).There are tools to catalyse deliberations and help problem-owners and stakeholders clarify their meanings and contextualise their values to the specific issues being considered (Keeney, 1992).Usually, problems falling into known contexts are well understood and there is little need to elicit or structure models to perform analyses.Problems falling into knowable space are usually well structured and problem owners’ values are also well understood. However, there may be a need to explore error structures in preparation to estimate parameters in the models (Gelman, 2003; Steed et al., 2013; Fekete and Primet, 2016).Many soft elicitation tools were developed for complex contexts: 'wicked' problems with deep uncertainties: e.g., soft systems, cognitive maps and similar tools to elicit perceptions of relationships between entities and problem owners' and stakeholder's values (Keeney, 1992; Rosenhead and Mingers, 2001).Soft elicitation tools and processes can be used to catalyse creative thinking about poorly understood contexts.