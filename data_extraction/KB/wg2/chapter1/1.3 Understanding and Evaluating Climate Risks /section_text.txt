Since 1990, IPCC assessments have included designated terms and other approaches for communicating the expert judgements made by authors (Mastrandrea and Mach, 2011). The goal of such methods has been consistent treatment of uncertainties in assessing and communicating the current state of knowledge. Because terms such as ‘probable’ or ‘likely ’ hold very different meanings to different people, a standardised approach is essential for enabling consistent interpretation (WGI Section 1.2.3.1). Since its 2001 assessment, IPCC authors have applied common guidance on expert judgement across the Working Groups (Moss and Schneider, 2000; IPCC, 2005). The AR5, iteratively building from past IPCC guidance, was the first report to apply a single framework consistently across the Working Groups and their diverse topics and associated disciplines (Figure 1.3; Mastrandrea et al., 2010; Mastrandrea and Mach, 2011). The outcome was increased comparability of assessment conclusions across the full spectrum of the physical science basis of climate change and resulting impacts, risks and responses (Mach et al., 2017).This framework for expert judgement is again being applied in the AR6 and associated special reports in the assessment cycle (Mastrandrea et al., 2010; see also WGI Box 1.1). Under the framework, the assessment of scientific understanding and uncertainties begins with evaluation of evidence and agreement —especially the type, amount, quality and consistency of evidence and the degree of agreement (steps 1–3 in Figure 1.6). Evidence assessed can reflect observations, experimental results, process-based understanding, statistical analyses or model outputs. Evidence is most robust when it consists of multiple lines of consistent, independent and high-quality evidence. The degree of agreement considers the extent of established, competing or speculative explanations for a given topic or phenomenon across the scientific community. Together, this evaluation of evidence and agreement forms a traceable account for each key finding in the assessment. Subsequently, the framework proceeds to evaluation of levels of confidence, which integrate evidence and agreement (steps 3–5 in Figure 1.6). Confidence reflects qualitative judgements of the validity of findings. It thereby facilitates, more readily, comparisons across assessment conclusions. Increasing evidence and agreement corresponds to increasing confidence (step 4 in Figure 1.6).Figure 1.6 | The IPCC AR5 and AR6 framework for applying expert judgement in the evaluation and characterisation of assessment findings. This illustration depicts the process assessment authors apply in evaluating and communicating the current state of knowledge. Guidance for the application of this framework is described in full detail in Mastrandrea et al. (2010). In addition to scientific knowledge, IK and LK is central to understanding and acting effectively on climate risk (Section 1.3.2.3). The diagram in this figure is reproduced from Mach et al. (2017).Open figure If uncertainties can be quantified, the framework involves a further option of characterising assessment findings with likelihood terms or more precise presentations of probability (steps 5–6 in Figure 1.6). The relevant probabilities can pertain to single events or broader outcomes. Probabilistic judgements can be based on statistical or modelling analyses, elicitation of expert views or other quantitative analyses. Where appropriate, authors can present probability more precisely with complete probability distributions or percentile ranges, also considering tails of distributions important for risk management. Usually, likelihood assignments are underpinned by high or very high confidence in the findings.Confidence is often most applicable in characterising key findings in WGII assessment (Mach et al., 2017). This tendency results from the diverse lines of evidence across disciplines relevant to climate change impacts, adaptation and vulnerability. By contrast, likelihood is more common in WGI assessment.The guidance to authors additionally identifies other practices and approaches relevant in applying expert judgement and developing assessment findings (Mastrandrea et al., 2010; Mastrandrea and Mach, 2011; Mach and Field, 2017). First, authors are encouraged to carefully consider appropriate generalisation within assessment findings, emphasising insights that are integrative, nuanced and rigorous (IAC, 2010; Mastrandrea et al., 2010; NEAA, 2010). Second, authors are instructed to attend to potential biases, including in group dynamics, such as tendencies towards overconfidence and anchoring or Type I (false positive) error aversion (Mastrandrea et al., 2010; Brysse et al., 2013; Anderegg et al., 2014; Morgan, 2014). Third, particular attention is drawn to the importance of evaluating and communicating ranges of potential outcomes to inform decision making and risk management (Mastrandrea et al., 2010). In some cases, deep uncertainties related to parameters or processes that are unknown or disagreed upon strongly benefit from dedicated methods of assessment and decision support (see Cross-Chapter Box DEEP in Chapter 17). Fourth, the guidance explores the different ways that framings of conclusions can shape their interpretation by readers. Finally, the guidance underscores the importance of reflecting upon all sources of uncertainty, which can include deep, difficult-to-quantify and easy-to-underestimate uncertainties arising from incomplete understanding of relevant processes or competing conceptualisations across the literature (Mastrandrea et al., 2010). A detailed review of literature assessing IPCC uncertainty characterisation methods is provided in WGI 1.2.3.1.