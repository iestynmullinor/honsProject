A key approach in climate science is the comparison of results from multiple model simulations with each other and against observations. These simulations have typically been performed by separate models with consistent boundary conditions and prescribed emissions or radiative forcings, as in the Coupled Model Intercomparison Project phases (CMIP, Meehl et al., 2000, 2007a; Taylor et al., 2012; Eyring et al., 2016). Such multi-model ensembles (MMEs) have proven highly useful in sampling and quantifying model uncertainty, within and between generations of climate models. They also reduce the influence on projections of the particular sets of parametrizations and physical components simulated by individual models. The primary usage of MMEs is to provide a well-quantified model range, but when used carefully they can also increase confidence in projections (Knutti et al., 2010). Presently, however, many models also share provenance (Masson and Knutti, 2011) and may have common biases that should be acknowledged when presenting and building on MME-derived conclusions (Section 1.5.4.6; Boé, 2018; Abramowitz et al., 2019).Since AR5, an increase in computing power has made it possible to investigate simulated internal variability and to provide robust estimates of forced model responses, using large initial condition ensembles (ICEs), also referred to as single model initial condition large ensembles (SMILEs). Examples using GCMs or ESMs that support assessments in AR6 include the CESM Large Ensemble (Kay et al., 2015), the MPI Grand Ensemble (Maher et al., 2019), and the CanESM2 large ensembles (Kirchmeier-Young et al., 2017). Such ensembles employ a single GCM or ESM in a fixed configuration, but starting from a variety of different initial states. In some experiments, these initial states only differ slightly. As the climate system is chaotic, such tiny changes in initial conditions lead to different evolutions for the individual realizations of the system as a whole. Other experiments start from a set of well-separated ocean initial conditions to sample the uncertainty in the circulation state of the ocean and its role in longer-time scale variations. These two types of ICEs have been referred to as ‘micro’ and ‘macro’ perturbation ensembles respectively (Hawkins et al., 2016). In support of this Report, most models contributing to CMIP6 have produced ensembles of multiple realizations of their historical and scenario simulations (Chapters 3 and 4).Recently, the ICE technique has been extended to atmosphere-only simulations (Mizuta et al., 2017), single-forcer influences such as volcanic eruptions (Bethke et al., 2017), regional modelling (Mote et al., 2015; Fyfe et al., 2017; Schaller et al., 2018; Leduc et al., 2019), and to attribution of extreme weather events using crowdsourced computing (climateprediction.net ; Massey et al., 2015).ICEs can also be used to evaluate climate model parameterizations, if models are initialized appropriately (Phillips et al., 2004; Williams et al., 2013), mostly within the framework of seamless weather and climate predictions (e.g., Palmer et al., 2008; Hurrell et al., 2009; Brown et al., 2012). Initializing an atmospheric model in hindcast mode and observing the biases as they develop permits testing of the parameterized processes, by starting from a known state rather than one dominated by quasi-random short-term variability (Williams et al., 2013; Ma et al., 2014; Vannière et al., 2014). However, single-model initial-conditions ensembles cannot cover the same degrees of freedom as a multi-model ensemble, because model characteristics substantially affect model behaviour (Flato et al., 2013).A third common modelling technique is the perturbed parameter ensemble (PPE; note that the abbreviation also sometimes refers to the sub-category ‘perturbed physics ensemble’). These methods are used to assess uncertainty based on a single model, with individual parameters perturbed to reflect the full range of their uncertainty (Murphy et al., 2004; Knutti et al., 2010; Lee et al., 2011; Shiogama et al., 2014). Statistical methods can then be used to detect which parameters are the main causes of uncertainty across the ensemble. PPEs have been used frequently in simpler models, such as EMICs, and are being applied to more complex models. A caveat of PPEs is that the estimated uncertainty will depend on the specific parameterizations of the underlying model and may well be an underestimation of the ‘true’ uncertainty. It is also challenging to disentangle forced responses from internal variability using a PPE alone.Together, the three ensemble methods (MMEs, ICEs, PPEs) allow investigation of climate model uncertainty arising from internal variability, initial and internal boundary conditions, model formulations and parameterizations (Parker, 2013). Figure 1.21 illustrates the different ensemble types. Recent studies have also started combining multiple ensemble types or using ensembles in combination with statistical analytical techniques. For example, Murphy et al. (2018) combine MMEs and PPEs to give a fuller assessment of modelling uncertainty. Wagman and Jackson (2018) use PPEs to evaluate the robustness of MME-based emergent constraints. Sexton et al. (2019) study the robustness of ICE approaches by identifying parameters and processes responsible for model errors at the two different time scales.Figure 1.21 | Illustration of common types of model ensemble, simulating the time evolution of a quantity Q (such as global mean surface temperature).  (a) Multi-model ensemble, where each model has its own realization of the processes affecting Q, and its own internal variability around the baseline value (dashed line). The multi-model mean (black) is commonly taken as the ensemble average.  (b) Initial condition ensemble, where several realizations from a single model are compared. These differ only by minute (‘micro’) perturbations to the initial conditions of the simulation, such that over time, internal variability will progress differently in each ensemble member.  (c) Perturbed physics ensemble, which also compares realizations from a single model, but where one or more internal parameters that may affect the simulations of Q are systematically changed to allow for a quantification of the impact of those quantities on the model results. Additionally, each parameter set may be taken as the starting point for an initial condition ensemble. In this figure, each set has three ensemble members. Open figureOverall, we assess that increases in computing power and the broader availability of larger and more varied ensembles of model simulations have contributed to better estimations of uncertainty in projections of future change (high confidence). Note, however, that despite their widespread use in climate science today, the cost of the ensemble approach in human and computational resources, and the challenges associated with the interpretation of multi-model ensembles, has been questioned (Palmer and Stevens, 2019; Touzé-Peiffer et al., 2020).