Techniques used for evaluating process-based climate models against observations were assessed in AR5 (Flato et al., 2013), and have progressed rapidly since (Eyring et al., 2019). The most widely used technique is to compare climatologies (long-term averages of specific climate variables) or time series of simulated (process-based) model output with observations, considering the observational uncertainty. A further approach is to compare the results of process-based models with those from statistical models. In addition to a comparison of climatological means, trends and variability, AR5 already made use of a large set of performance metrics for a quantitative evaluation of the models.Since AR5, a range of studies has investigated model agreement with observations well beyond large-scale mean climate properties (e.g., Bellenger et al., 2014; Covey et al., 2016; Pendergrass and Deser, 2017; Goelzer et al., 2018; Beusch et al., 2020a), providing information on the performance of recent model simulations across multiple variables and components of the Earth system (e.g., Anav et al., 2013; Guan and Waliser, 2017). Based on such studies, this Report assesses model improvements across different CMIP DECK, CMIP6 historical and CMIP6-Endorsed MIP simulations, and of differences in model performance between different classes of models, such as high- versus low-resolution models (see e.g., Section 3.8.2).In addition, process- or regime-oriented evaluation of models has been expanded since AR5. By focusing on processes, causes of systematic errors in the models can be identified and insights can be gained as to whether a mean state or trend is correctly simulated and for the right reasons. This approach is commonly used for the evaluation of clouds (e.g., Williams and Webb, 2009; Konsta et al., 2012; Bony et al., 2015; Dal Gesso et al., 2015; Jin et al., 2017), dust emissions (e.g., Parajuli et al., 2016; Wu et al., 2016) as well as aerosol–cloud (e.g., Gryspeerdt and Stier, 2012) and chemistry–climate (SPARC, 2010) interactions. Process-oriented diagnostics have also been used to evaluate specific phenomena such as the El Niño–Southern Oscillation (ENSO; Guilyardi et al., 2016), the Madden–Julian Oscillation (MJO; Ahn et al., 2017; Jiang et al., 2018), Southern Ocean clouds (Hyder et al., 2018), monsoons (Boo et al., 2011; James et al., 2015) and tropical cyclones (Kim et al., 2018).Instrument simulators provide estimates of what a satellite would see if looking down on the model-simulated planet, and improve the direct comparison of modelled variables such as clouds, precipitation and upper tropospheric humidity with observations from satellites (e.g., Kay et al., 2011; Klein et al., 2013; Cesana and Waliser, 2016; Konsta et al., 2016; Jin et al., 2017; Chepfer et al., 2018; Swales et al., 2018; Zhang et al., 2018). Within the framework of the Cloud Feedback Model Intercomparison Project (CFMIP) contribution to CMIP6 (Webb et al., 2017), a new version of the Cloud Feedback Model Intercomparison Project Observational Simulator (COSP; Swales et al., 2018) has been released which makes use of a collection of observation proxies or satellite simulators. Related approaches in this rapidly evolving field include simulators for Arctic Ocean observations (Burgard et al., 2020) and measurements of aerosol observations along aircraft trajectories (Watson-Parris et al., 2019).In this Report, model evaluation is performed in the individual chapters, rather than in a separate chapter as was the case for AR5. This applies to the model types discussed above, and also to dedicated models of subsystems that are not (or not yet) part of usual climate models, for example, glacier or ice-sheet models (Annex II). Further discussions are found in Chapter 3 (attribution), Chapter 5 (carbon cycle), Chapter 6 (short-lived climate forcers), Chapter 8 (water cycle), Chapter 9 (ocean, cryosphere and sea level), Chapter 10 (regional scale information) and the Atlas (regional models).