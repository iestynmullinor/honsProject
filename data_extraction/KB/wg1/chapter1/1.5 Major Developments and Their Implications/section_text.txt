Assessments of climate model ensembles have commonly assumed that each individual model is of equal value (‘model democracy’) and when combining simulations to estimate the mean and variance of quantities of interest, they are typically unweighted (Haughton et al., 2015). This practice has been noted to diminish the influence of models exhibiting a good match with observations (Tapiador et al., 2020). However, exceptions to this approach exist, notably AR5 projections of sea ice, which only selected a few models which passed a model performance assessment (Collins et al., 2013), and more studies on this topic have appeared since AR5 (e.g., Eyring et al., 2019). Ensembles are typically sub-selected by removing either poorly performing model simulations (McSweeney et al., 2015) or model simulations that are perceived to add little additional information, typically where multiple simulations have come from the same model. They may also be weighted based on model performance.Several recent studies have attempted to quantify the effect of various strategies for selection or weighting of ensemble members based on some set of criteria (Haughton et al., 2015; Olonscheck and Notz, 2017; Sanderson et al., 2017). Model weighting strategies have been further employed since AR5 to reduce the spread in climate projections for a given scenario by using weights based on one or more model performance metrics (Wenzel et al., 2016; Knutti et al., 2017; Sanderson et al., 2017; Lorenz et al., 2018; Liang et al., 2020). However, models may share representations of processes, parameterization schemes, or even parts of code, leading to common biases. The models may therefore not be fully independent, calling into question inferences derived from multi-model ensembles (Abramowitz et al., 2019). Emergent constraints (Section 1.5.4.5) also represent an implicit weighting technique that explicitly links present performance to future projections (Bracegirdle and Stephenson, 2013).Concern has been raised about the large extent to which code is shared within the CMIP5 multi-model ensemble (Sanderson et al., 2015a). Boé (2018) showed that a clear relationship exists between the number of components shared by climate models and how similar the simulations are. The resulting similarities in behaviour need to be accounted for in the generation of best-estimate multi-model climate projections. This has led to calls to move beyond equally-weighted multi-model means towards weighted means that take into account both model performance and model independence (Sanderson et al., 2015b, 2017; Knutti et al., 2017). Model independence has been defined in terms of performance differences within an ensemble (Masson and Knutti, 2011; Knutti et al., 2013, 2017, Sanderson et al., 2015a, b, 2017; Lorenz et al., 2018). However, this definition is sensitive to the choice of variable, observational dataset, metric, time period, and region, and a performance-ranked ensemble has been shown to sometimes perform worse than a random selection (Herger et al., 2018a). The adequacy of the constraint provided by the data and experimental methods can be tested using a ‘calibration-validation’ style partitioning of observations into two sets (Bishop and Abramowitz, 2013), or a ‘perfect model approach’ where one of the ensemble members is treated as the reference dataset and all model weights are calibrated against it (Bishop and Abramowitz, 2013; Wenzel et al., 2016; Knutti et al., 2017; Sanderson et al., 2017; Herger et al., 2018a, b). Sunyer et al. (2014) use a Bayesian framework to account for model dependencies and changes in model biases. Annan and Hargreaves (2017) provides a statistical, quantifiable definition of independence that is independent of performance-based measures.The AR5 quantified uncertainty in CMIP5 climate projections by selecting one realization per model per scenario, and calculating the 5–95% range of the resulting ensemble (Box 4.1) and the same strategy is generally still used in AR6. Broadly, the following chapters take the CMIP6 5–95% ensemble range as the likely uncertainty range for projections, 8 with no further weighting or consideration of model ancestry and as long as no universal, robust method for weighting a multi-model projection ensemble is available (Box 4.1). A notable exception to this approach is the assessment of future changes in global surface air temperature (GSAT), which also draws on the updated best estimate and range of equilibrium climate sensitivity assessed in Chapter 7. For a thorough description of the model-weighting choices made in this Report, and the assessment of GSAT, see Chapter 4 (Box 4.1). Model selection and weighting in downscaling approaches for regional assessment is discussed in Chapter 10 (Section 10.3.4).