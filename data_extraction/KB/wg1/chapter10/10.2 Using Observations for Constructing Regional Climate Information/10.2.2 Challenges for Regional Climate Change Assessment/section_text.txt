Beyond climate monitoring, the quality and availability of multiple observational reference datasets play a central role in model evaluation. In fact, when using observations for model evaluation, there are multiple examples where inter-observational uncertainty is as large as the inter-model variability. This has been shown for various aspects of the Indian monsoon (Section 10.6.3; Collins et al., 2013a) and for precipitation uncertainties over Africa (Section 10.6.4; Nikulin et al., 2012; Sylla et al., 2013; Dosio et al., 2015; Bador et al., 2020) and Europe (Prein and Gobiet, 2017). Kotlarski et al. (2019) compared three high-resolution observational temperature and precipitation datasets (E-OBS, a compilation of national/regional high-resolution gridded datasets, and the EURO4M-MESAN 0.22Â° reanalysis based on a high-resolution limited-area model) with five EURO-CORDEX RCMs driven by ERA-Interim. Generally, the differences between RCMs are larger than those between observation datasets, but for individual regions and performance metrics, observational uncertainty can dominate. They also showed that the choice of reference dataset can have an influence on the RCM performance score. Over the high mountain Asia region and East Asia, differences among gridded precipitation datasets can generate significant uncertainties in deriving precipitation characteristics (J. Kim et al., 2015; Kim and Park, 2016; Guo et al., 2017). Over western North America, observational uncertainty induces differences in multi-decadal precipitation trends (Lehner et al., 2018). Taking a very different perspective, the agreement between model simulations may be used to estimate the uncertainty and quality of observations (Massonnet et al., 2016). There is  high confidence that an ensemble of multiple observational references at a regional scale is fundamental for model performance assessment. The uncertainties vary according to region, season, and statistical properties (Cross-Chapter Box 10.2).