Data, either from observations or models, is in general not inherently information, but may contain relevant information if interpreted appropriately (Hewitson et al., 2017). The same applies to other sources of climate information. Relevance is controlled by the given user context (Section 10.5.2.1) and relates to the required temporal and spatial scales (Section 10.5.2.3), the characteristics of required variables (often referred to as indicators), and the meteorological and climatic phenomena driving these variables (Section 10.1.3). For example, if climate information for driving impact models is sought (e.g., McSweeney et al., 2015), the impact modelling analysis in the target region is the specific user context.Climate risk assessment considers all plausible outcomes (Weaver et al., 2017; Marchau et al., 2019; Sutton, 2019). Thus, a key element of information construction is the exploration and reconciliation of different sources of information (Barsugli et al., 2013; Hewitson et al., 2014b; Maraun and Widmann, 2018b) and involves mainly two issues: first, assessing the fitness of different sources in the given context and thereby potentially omitting (or down-weighting) selected sources (Sections 10.3.3), and, second, integrating different sources into a broader picture within a context (Sections 10.3.4).A non-comprehensive selection of approaches that may contribute to the construction of information includes:Overall assessment and intercomparison of different sources of information, including hierarchies of models and identification of potentially conflicting results (Figure 10.16), where observational availability plays a critical role (Section 10.2.3).Assessing the emergence of forced trends from internal variability (Section 10.4.3), and testing whether differences in simulations can be explained by internal variability, ideally using initial-condition large ensembles (Sections 10.3.4.3 and 10.4.3).Assessing the interdependence of chosen models to identify the amount of independent information (Section 10.3.4.4).Process-based evaluation with focus on those processes that are relevant for the specific application (Sections 10.3.3.4–10.3.3.10).Weighting or sub-selecting ensembles based on a priori knowledge or the outcome of a process-based evaluation, while sampling as much uncertainty as possible (Section 10.3.4.4).Tracing back differences in projections to the representation of fundamental processes, for example, by using physical climate storylines (Sections 10.3.4.2 and Box 10.2) or sensitivity simulations (Section 10.3.2.3).Producing physical-climate storylines (Box 10.2) to explore uncertainties not sampled by available model ensembles (Shepherd et al., 2018), for example in pseudo-global warming experiments (Section 10.3.2.2), or to simulate events that have never happened before but are nevertheless plausible (Lin and Emanuel, 2016).Attributing observed changes to different external forcings and internal drivers (Section 10.4.1).Comparing observed trends with past simulated trends in order to constrain projections with, for instance, the Allen–Stott–Kettleborough method (Allen et al., 2000; Stott and Kettleborough, 2002; Stott et al., 2013) to explain drivers of past observed trends (Section 10.4.2) for understanding future trends.Integrating present-day performance via emergent constraints to reduce projection uncertainty (Section 10.3.2).Complementing the observational and model-based sources with expert judgement (e.g., integrating knowledge from theory or experience that is available from experts or the literature; Section 10.5.1).These approaches often can be used in combination to increase confidence in conclusions drawn (Hewitson et al., 2017).